# ZVIR âš¡  
**Zero-Shot Implicit Deep Image Prior with Prior Activation for Infrared and Visible Image Fusion**

[![Paper](https://img.shields.io/badge/Paper-Pattern%20Recognition-blue)](https://www.sciencedirect.com/science/article/abs/pii/S0031320325013299)
[![DOI](https://img.shields.io/badge/DOI-10.1016%2Fj.patcog.2025.112666-critical)](https://doi.org/10.1016/j.patcog.2025.112666)
![Python](https://img.shields.io/badge/Python-3.8%2B-green)
![Pytorch](https://img.shields.io/badge/DIP-orange)
![Status](https://img.shields.io/badge/ZeroShot-Fusion-9cf)

ğŸ“Note: This code is intended primarily for academic research and reproduction of paper results.

# ğŸ“Œ Abstract
ğŸ“Œ Infrared and visible image fusion aims to generate an image that simultaneously contains features from both modalities, thereby enhancing its information richness and expressive capability. Although the deep image prior has demonstrated excellent performance in traditional inverse problems such as image restoration and denoising, conventional network structures struggle to effectively capture the joint features required for multimodal image fusion. To address this issue, we propose a fusion framework that takes the visible image as the primary degraded input and models the fusion task as the reconstruction of a fused feature image based on the visible modality. This framework leverages the internal recurrence of the fused image and introduces connection modules during the downsampling stage to enhance infrared feature representation, thereby utilizing the network structure to form a deep image prior that guides the fusion process. Experimental results further demonstrate that although the network is affected by the degraded input, its structure can still implicitly extract effective priors required for image fusion. By replacing the input image in the network path, the deep image prior can be activated to generate the target fused image. We perform both qualitative and quantitative comparisons with mainstream methods on several public infrared-visible datasets, demonstrating the effectiveness of the proposed method in terms of fusion quality and downstream task performance.

# ğŸ“Œ Contributions
â€¢ To the best of our knowledge, this is the first work to apply deep image prior to the task of infrared and visible image fusion, where the network structure is leveraged to extract deep image prior for achieving zero-shot fusion.  
â€¢ A novel network architecture is proposed, in which feature priors are progressively enhanced through connection modules, allowing the network to establish different priors based on the information contained in the images themselves.  
â€¢ This research validates that the network architecture is capable of implicitly capturing deep image prior. The inhibitory effect of shallow layers on these priors can be mitigated by introducing perturbations to the input path, which facilitates the activation of deep image prior and enables the generation of the final fused image.  
â€¢ Extensive qualitative and quantitative experiments are conducted on multiple public infrared-visible datasets, validating the effectiveness of the proposed method in both fusion quality and downstream ta


# ğŸ§  FrameworkğŸ“ 
 
<p align="center">
  <img src="Introduction/ZVIR_S.png" width="800">
</p>
<p align="center">
  <img src="Introduction/moudule.png" width="800">
</p>

# ğŸ” Deep Image Prior And Prior Activation
ğŸ“Œ In our proposed method, the infrared features are propagated only through the input path. We design a preference-aware loss function to ensure that the infrared prior is implicitly formed by the network architecture itself. We further compare our approach with the traditional DIP framework, where the DIP directly uses loss functions to extract bi-modal features and the network input is pure noise. A comparative analysis between the two is conducted.
<p align="center">  
 <img src="Introduction/prior_act.png" width="500">
</p>
ğŸ” The feature map outputs of each layer in the network architecture are visualized, intuitively demonstrating how the network suppresses infrared information. For comparison, the visualizations of prior activation are also presented, showing how both the prior and fused features are expressed within the architecture during the activation process.
<p align="center">
  <img src="Introduction/dip_ours.png" width="500">  
</p>

# ğŸ“ˆ Reproduction

## âš™ï¸ Parameter Settings
ğŸ“Œ In our method, there are two primary parameters to be configured: the number of iterations and the parameter Î±. The former determines the running time of the network, while the latter controls the amount of infrared information preserved within the network. Here, we provide the recommended reference settings based on our experimental results.
<p align="center">
 <img src="Introduction/iter.png" width="500">  
</p>
<p align="center">
<img src="Introduction/alpha.png" width="500">
</p>

## ğŸ” Limitations
ğŸ“Œ Since our method relies on deep prior, both modalities are extracted directly through the network architecture itself. When the infrared and visible image edges are highly similar, or when the visible image is affected by low illumination, the fused result may be less discriminative. In such cases, the value of Î± needs to be adjusted, similar to manually increasing infrared preference, so that more infrared information can be introduced to enhance the fusion quality.

## âš™ï¸ Run Code
ğŸ“Œ python  train.py
